import typing
from abc import abstractmethod, ABC

import numpy as np
from aalpy.automata import Mdp, MdpState

from evaluate.metrics.Metric import Metric
from evaluate.utils import get_traces_from_mdp, print_table, is_deterministically_labeled, logger
from HierarchicMDP import Input, Output


class Compliance(Metric, ABC):
    """
    Compliance, as introduced in a reinforcement learning context by Lazaric et al. in 2008 [1][2]
    and described for two MDPs by Garcia et al. in [3].

    Quote from Garcia et al. [3]:

        Definition 5
            Given two MDPs M_i and M_j, and a set of experience tuples generated in M_i,
            D_Mi = {τ_0,...τ_m} with τ_k = <s_k, a_k, s'_k, r_k>, the probability of an
            experience tuple σ = <s,a,s',r> in the task M_j of being generated by M_i is
            defined as:
                P(σ|D_M_i) = (T_i)^{a}_{ss'} * (R_i)^{a}_{sr}
            where
                (T_i)^{a}_{ss'} is the probability of transitioning to s'
            and
                (R_i)^{a}_{sr} is the probability of generating the reward r
            after executing the action a in state s in M_i

        Definition 5 provides a formal description of the probability of a sample σ generated in
        an MDP M_j of having been generated in an MDP M_i. If instead of having a single σ tuple
        of the target task M_j, we have a set of tuples, D_Mj, Definition 5 could be used to compute
        the compliance between the entire target task M_j and the entire source task M_i by repeating
        this operation for all samples in D_Mj.

        Definition 6
            Given two MDPs, M_i and M_j, and two sets of experience tuples generated D_Mi and D_Mj
            gathered from M_i and M_j, the compliance between M_i and M_j is computed as:
                𝚲 = 1/n * \Sigma^{n}_{t=0} P(σ_t|D_Mi)
            where n is the number of samples in D_Mj, and σ_t is the t-th tuple in T_Mj

        𝚲 is not strictly a distance metric, but a probability: the more likely the samples of the target
        task are generated in the source task, the closer 𝚲 to 1. Therefore, _compliance_ could be used
        to obtain a distance metric between MDPs like the ones this survey is looking for, e.g.
            d(M_i,M_j) = 1 - 𝚲

    Since we don't work with MDPs with reward here, but with deterministically labeled MDPs, we can
    drop (R_i)^{a}_{sr} from definition 5 entirely, and solely focus on the probability of transitioning
    from state s to state s' given an input symbol a (where the state s' can be uniquely identified
    by its output symbol).

    In this context, we (mostly) don't have two actual MDPs we want to compare, but rather a
    "Ground Truth System" (GTS), and two learned models of this system, M_1 and M_2, and we
    want to find out which of these two models more closely/accurately models the GTS.
    For this, we compare the probability of a trace (sample) σ from the GTS being produced in M_1
    with the probability of this sample being produced in M_2.


    [1]: http://researchers.lille.inria.fr/~lazaric/Webpage/Publications_files/lazaric2008transfer.pdf
    [2]: http://researchers.lille.inria.fr/~lazaric/Webpage/Publications_files/lazaric2008knowledge.pdf
    [3]: https://link.springer.com/article/10.1007/s10994-022-06242-4
    """
    def calculate(self, num_traces=1000, trace_len=100, normalize_per_step=True):
        logger.evaluating(f"Calculating {type(self).__name__}...")
        deterministically_labeled = True
        if not is_deterministically_labeled(self.learned_model_alg):
            logger.debug(f"The MDP learnt with {self.algorithm_name} is not deterministically labeled!")
            deterministically_labeled = False
        if not is_deterministically_labeled(self.learned_model_hal):
            logger.debug(f"The MDP learnt with HAL is not deterministically labeled!")
            deterministically_labeled = False

        if deterministically_labeled:
            compliance = self.compliance(
                ground_truth_model=self.original_mdp,
                models=(self.learned_model_alg, self.learned_model_hal),
                num_traces=num_traces,
                trace_len=trace_len
            )
        else:
            compliance = self.compliance_nondeterministically_labeled(
                ground_truth_model=self.original_mdp,
                models=(self.learned_model_alg, self.learned_model_hal),
                num_traces=num_traces,
                trace_len=trace_len,
            )
            # TODO use normal compliance for ALG (if det labeled) and assert equality!

        self.print_compliance_dict(compliance, num_traces, trace_len)

        return {"HAL": compliance[self.learned_model_hal],
                self.algorithm_name: compliance[self.learned_model_alg]}

    @staticmethod
    def compliance(ground_truth_model: Mdp, models: typing.Sequence[Mdp], num_traces=100, trace_len=100) -> dict[Mdp, float]:
        """
        Calculate compliance of each of the models in @models to random traces produced by @ground_truth_system,
        and return a dictionary mapping model -> compliance.

        :param ground_truth_model  the ground truth mdp from which random traces are samples, whose compliance in
                                    the models is calculated
        :param models               list of models to calculate compliance for
        :param num_traces           how many traces to compute & average over
        :param trace_len            the length of each individual trace

        :returns                    a dict mapping from model to compliance (float)
        """

        # get traces from GTS
        traces = get_traces_from_mdp(ground_truth_model, num_traces, trace_len)

        # a list of each step's probability for model
        step_probabilities = {model: [] for model in models}

        # go through every trace
        for trace in traces:

            # reset models to initial state
            for model in models:
                model.reset_to_initial()
                assert trace[0] == model.initial_state.output, f"The initial states of trace and model don't match!"

            # go through every step of this trace and calculate its probability
            for i, (input, output) in enumerate(trace[1:]):
                for model in models:
                    if i > 0 and step_probabilities[model][-1] == 0:
                        # if the probability of the last step was 0, the probability of each following step is also 0,
                        step_probabilities[model].append(0)
                    else:
                        # calculate the probability of the step and take it
                        probability = Compliance.do_transition_and_return_probability(model, input, output)
                        step_probabilities[model].append(probability)

        # calculate "whole" compliance by averaging these
        return {model: np.average(step_probabilities[model]) for model in models}

    @classmethod
    def compliance_nondeterministically_labeled(cls, ground_truth_model: Mdp, models: typing.Sequence[Mdp],
                                                num_traces=100, trace_len=100) -> dict[Mdp, float]:
        """
        Calculate compliance of each of the models in @models to random traces produced by @ground_truth_system,
        and return a dictionary mapping model -> compliance.

        :param ground_truth_model  the ground truth system from which random traces are samples, whose compliance in
                                    the models is calculated
        :param models               list of models to calculate compliance for
        :param num_traces           how many traces to compute & average over
        :param trace_len            the length of each individual trace

        :returns                    a dict mapping from model to compliance (float)
        """
        # get traces from GTS
        traces = get_traces_from_mdp(ground_truth_model, num_traces, trace_len)

        # a list of each step's probability for model
        step_probabilities = {model: [] for model in models}

        # go through every trace
        for trace in traces:

            # reset models to initial state
            for model in models:
                model.reset_to_initial()
                assert trace[0] == model.initial_state.output, f"The initial states of trace ('{trace[0]}') and model ('{model.current_state.output}') don't match!"

            # list of possible current states and their probability after last step for model
            possible_current_states = {model: [(model.current_state, 1.0)] for model in models}

            # go through every step of this trace and calculate its probability
            for i, (input, output) in enumerate(trace[1:]):
                for model in models:
                    if i > 0 and step_probabilities[model][-1] == 0:
                        # if the probability of the last step was 0, the probability of each following step is also 0
                        step_probabilities[model].append(0)
                    else:
                        next_states = []
                        probability = 0.0

                        # go through every possible current state, and the possibility of being in that state
                        for possible_current_state, probability_of_being_in_state in possible_current_states[model]:
                            # get all possible next states and their probabilities
                            possible_transitions = cls.get_possible_transitions(possible_current_state, input, output)

                            cumulative_prob = sum(p for s, p in possible_transitions)
                            next_states.extend([(s, (p/cumulative_prob)*probability_of_being_in_state if cumulative_prob else 0) for s,p in possible_transitions])

                            # sum up the probabilities of all possible next states from this state and multiply them with the probability of being in this stae
                            prob_from_this_state = sum(p for s,p in possible_transitions) * probability_of_being_in_state
                            probability += prob_from_this_state

                        # de-duplicate next_states
                        seen = {}
                        for state, prob in next_states:
                            if state not in seen:
                                seen[state] = prob
                            else:
                                seen[state] += prob
                        next_states = [(state, prob) for (state, prob) in seen.items()]

                        if cls.normalize_per_step():
                            # normalize next_states such that the cumulative probability gives 1.0 (or 0.0 if no transitions)
                            # -> the probability of doing *any* transition to a state with this output, via this input, from any of the possible current states
                            cumulative_prob = sum(p for s, p in next_states)
                            for i in range(len(next_states)):
                                state, prob = next_states[i]
                                next_states[i] = (state, prob / cumulative_prob)

                            assert any(np.isclose(sum(p for (s, p) in next_states), [1, 0]))

                        step_probabilities[model].append(probability)
                        possible_current_states[model] = next_states


        # now we have a compliance-value for each trace in traces
        # calculate "whole" compliance by averaging these
        return {model: np.average(step_probabilities[model]) for model in models}

    @staticmethod
    def do_transition_and_return_probability(model: Mdp, input: Input, output: Output) -> float:
        """
        Do the transition from the current state of the model with input symbol @input to the state with output @output,
        and return the probability of this transition.
        Since we only work with deterministically labelled MDPs, we expect at most one state to be reachable from @state
        with @input to have output @output, i.e. a maximum of one possible transition, and raise an exception otherwise.

        :returns the probability of the transition from @state to a state with output @output via input symbol @input
        """
        next_state, probability = Compliance.get_transition(model.current_state, input, output)
        if next_state is not None:
            model.current_state = next_state
        return probability

    @staticmethod
    def get_transition(state: MdpState, input: Input, output: Output) -> tuple[MdpState, float]:
        """
        Returns the probability of a transition from state @state with the input symbol @input into a state with output @output,
        and the next state after this transition is taken.
        Only works for deterministically labeled MDPs; we expect at most one state to be reachable from @state
        with @input to have output @output, i.e. a maximum of one possible transition, and raise an exception otherwise.

        :returns a tuple of (next_state, probability)
        """
        possible_transitions = Compliance.get_possible_transitions(state, input, output)
        match len(possible_transitions):
            case 1:
                next_state, probability = possible_transitions[0]
            case 0:
                next_state = None
                probability = 0.0
            case _:
                raise RuntimeError("The MDP seems not to be deterministically labelled!")

        return next_state, probability

    @staticmethod
    def get_possible_transitions(state: MdpState, input: Input, output: Output) -> list[tuple[MdpState, float]]:
        return [(state, prob) for (state, prob) in state.transitions[input] if state.output == output]

    @staticmethod
    @abstractmethod
    def normalize_per_step() -> bool:
        pass

    def print_compliance_dict(self, compliance: dict[Mdp, float], num_traces: int, trace_len: int):
        logger.info(f"-------------------- {self.name.upper()} --------------------")
        logger.info(f"Compliance of a trace t produced by the original system being produced in the models, "
              f"averaged over {num_traces} traces of lenght {trace_len} each.\n"
              f"The closer compliance is to 1, the more likely the trace is produced by the model.")

        print_table([
            ["Hierarchical MDP", self.algorithm_name],
            [compliance[self.learned_model_hal], compliance[self.learned_model_alg]]
        ])


class ComplianceStepNormalized(Compliance):
    @staticmethod
    def normalize_per_step() -> bool:
        return True

class ComplianceNotStepNormalized(Compliance):
    @staticmethod
    def normalize_per_step() -> bool:
        return False

